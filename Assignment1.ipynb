{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac98daef",
   "metadata": {},
   "source": [
    "#### <mark>**Problem 1**</mark>\n",
    "\n",
    "MDP Definition\n",
    "An MDP is defined by the tuple: (S, A, P, R, γ)\n",
    "\n",
    "* S is State Space (S)\n",
    "* A is Action Space (A)\n",
    "* P is a state transition probability matrix \n",
    "$$P_{ss'} = \\mathbb{P}[S_{t+1} = s' \\mid S_t = s, A_t = a]$$\n",
    "* R is a reward function \n",
    "$$R_s = \\mathbb{E}[R_{t+1} \\mid S_t = s, A_t = a]$$\n",
    "* γ is a discount factor\n",
    "$$\\gamma \\in [0, 1]$$\n",
    "\n",
    "___\n",
    "\n",
    "**The State Space (S)**\n",
    "The state must provide a complete snapshot of the system's physical configuration so the agent can predict the outcome of its actions.\n",
    "* Joint Positions: The current position of each linkage (angles or displacements).\n",
    "* Joint Velocities: Necessary to account for momentum and ensure smoothness (penalizing jerk or acceleration).\n",
    "* End-Effector Position: The (x, y, z) coordinates of the \"hand\" relative to the target object (position + orientation). End-effector pose can be derived from joint states, but including it improves learning stability.\n",
    "* Object Status: Position of the object to be picked.\n",
    "* Gripper Status: Whether the gripper has successfully secured the object (open/closed or force)\n",
    "\n",
    "Reasoning: \n",
    "* Without velocity data, the agent cannot perceive \"smoothness\" or deceleration, leading to jerky movements or overshooting.\n",
    "* Explicit goal information allows generalization across different pick-and-place locations.\n",
    "* Positions + velocities fully describe the arm’s dynamics.\n",
    "\n",
    "___\n",
    "\n",
    "**The Action Space (A)**\n",
    "Because we want fast and smooth movements, actions should be continuous and low-level. We avoid discrete steps (like Up, Down, Right, Left).\n",
    "* Torque Control (τ): The agent sends a specific amount of electric current/force to each motor at every time step: $$a_t = \\tau_t$$\n",
    "    - Pros:\n",
    "        1. Maximum control authority\n",
    "        2. Learns smooth, dynamic motions\n",
    "\n",
    "    - Cons: Harder to learn, requires more data\n",
    "\n",
    "* Alternative (Velocity Control): Specifying desired angular velocities, though torque control is the \"gold standard\" for truly fluid, human-like motion.\n",
    "\n",
    "    - Pros: \n",
    "        1. More stable learning\n",
    "        2. Still supports smooth movements\n",
    "\n",
    "Reasoning: \n",
    "Controlling torques allows the agent to utilize gravity and inertia, which is essential for high-speed efficiency.\n",
    "\n",
    "___\n",
    "\n",
    "**The Reward Function (R)**\n",
    "The reward function is the most critical part of the design. It must balance competing goals: accuracy, speed, and mechanical preservation. A composite reward function might look like this:\n",
    "$$R = W_1(R_{\\text{task}}) - W_2(R_{\\text{energy}}) - W_3(R_{\\text{jerk}})$$\n",
    "\n",
    "Objectives: \n",
    "1. Reach the target accurately (accuracy)\n",
    "2. Do it quickly (Speed)\n",
    "3. Move smoothly and avoid excessive forces or unsafe behaviour\n",
    "\n",
    "| Component | Definition | Reasoning |\n",
    "| :--- | :--- | :--- |\n",
    "| **Task Success** | Large positive reward for placing the object; negative penalty based on distance to target. | Drives the agent toward the primary goal. |\n",
    "| **Energy Penalty** | A penalty proportional to the square of the torques applied. | Encourages efficiency and prevents \"over-powering\" the motors. |\n",
    "| **Smoothness (Jerk)** | A penalty on the derivative of acceleration (change in torque). | **Crucial for smoothness.** It penalizes sudden, vibrating movements that cause wear and tear. |\n",
    "| **Time Penalty** | A small negative reward for every step taken. | Incentivizes the \"fast\" requirement. |\n",
    "___\n",
    "\n",
    "**The Transition Dynamics (P) and Environment**\n",
    "* Transition Function: $$\\mathbb{P}(s_{t+1} \\mid s_t, a_t)$$\n",
    "* Time Step (∆t): Must be very small (e.g., 1ms to 10ms). High-speed control requires a high control frequency to react to physical oscillations.\n",
    "* Physics Engine: The environment would likely be a simulator like MuJoCo or Isaac Gym, which models gravity, friction, and Coriolis forces before deploying to a physical arm.\n",
    "\n",
    "___\n",
    "\n",
    "**Episode Termination**\n",
    "An episode ends when one of the following items is true:\n",
    "* Object successfully placed in the target place\n",
    "* Or the maximum time steps are exceeded \n",
    "* Or safety constraint violated (collision, joint limit)\n",
    "\n",
    "___\n",
    "\n",
    "**Discount factor γ**\n",
    "* Typically $$\\gamma \\in [0.95, 0.99]$$\n",
    "* Encourages planning smooth trajectories rather than greedy moves\n",
    "* High discount factor helps capture delayed rewards from smooth control\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe9e58c",
   "metadata": {},
   "source": [
    "#### <mark>**Problem 2**</mark>\n",
    "\n",
    "**Setup and Assumptions**\n",
    "To perform Value Iteration, we use the Bellman Optimality Equation.The update rule for a state $s$ at iteration $k+1$ is:$$V_{k+1}(s) = \\max_{a} \\left( R(s, a) + \\gamma \\sum_{s'} P(s' | s, a) V_k(s') \\right)$$\n",
    "\n",
    "* Rewards(R): The problem states $R(s)$ is constant for all actions, so $R(s, a) = R(s)$.\n",
    "    - $R(s1) = 5$\n",
    "    - $R(s2) = 10$\n",
    "    - $R(s3) = 1$\n",
    "    - $R(s4) = 2$\n",
    "\n",
    "* Transitions: Deterministic. If an action moves into a wall, $s' = s$.\n",
    "* Discount Factor ($\\gamma$): A discount factor is required for the values to converge in a continuing task. Since one was not specified in the prompt, I will assume a standard value of $\\gamma = 0.9$.\n",
    "* Initialization: We initialize the value of all states to 0.$$V_0(s) = 0 \\text{ for all } s$$\n",
    "* Note on Initial Policy: Value Iteration searches for the optimal policy directly by updating values. It does not require an initial policy (unlike Policy Iteration). Therefore, the \"Initial Policy $\\pi(up|s)=1$\" is ignored for the initialization step.\n",
    "\n",
    "____\n",
    "\n",
    "**Iteration 1**\n",
    "In the first iteration, we update the values using $V_0$ (which is all zeros).\n",
    "1. Equation ApplicationSince $V_0(s') = 0$ for all states, the term $\\gamma V_0(s')$ becomes 0. The equation simplifies to just the immediate reward:$$V_1(s) = \\max_{a} [R(s) + 0] = R(s)$$\n",
    "2. Value Function UpdatesState s1: $V_1(s1) = 5$State s2: $V_1(s2) = 10$State s3: $V_1(s3) = 1$State s4: $V_1(s4) = 2$\n",
    "3. Updated Value Function ($V_1$)\n",
    "\n",
    "| State | V1​(s) | \n",
    "| :--- | :----: | \n",
    "| s1    | 5.0   |\n",
    "| s2    | 10.0  |\n",
    "| s3    | 1.0   |\n",
    "| s4    | 2.0   |\n",
    "\n",
    "___\n",
    "\n",
    "**Iteration 2**\n",
    "Now we calculate $V_2$ using the values from $V_1$.\n",
    "\n",
    "Formula: $V_{2}(s) = R(s) + 0.9 \\times \\max_{a} V_1(s')$.\n",
    "\n",
    "Value Function Updates (Step-by-Step)\n",
    "\n",
    "State s1 (R=5):\n",
    "* Up: Hits wall $\\to$ stays in s1 ($V_1=5$). Value: $5 + 0.9(5) = 9.5$\n",
    "* Down: Moves to s3 ($V_1=1$). Value: $5 + 0.9(1) = 5.9$\n",
    "* Left: Hits wall $\\to$ stays in s1 ($V_1=5$). Value: $5 + 0.9(5) = 9.5$\n",
    "* Right: Moves to s2 ($V_1=10$). Value: $5 + 0.9(10) = 14.0$\n",
    "* Max: The best action is Right.\n",
    "* Update: $$V_2(s1) = 14.0$$\n",
    "\n",
    "State s2 (R=10):\n",
    "* Up: Hits wall $\\to$ stays in s2 ($V_1=10$). Value: $10 + 0.9(10) = 19.0$\n",
    "* Down: Moves to s4 ($V_1=2$). Value: $10 + 0.9(2) = 11.8$\n",
    "* Left: Moves to s1 ($V_1=5$). Value: $10 + 0.9(5) = 14.5$\n",
    "* Right: Hits wall $\\to$ stays in s2 ($V_1=10$). Value: $10 + 0.9(10) = 19.0$\n",
    "* Max: The best actions are Up or Right.\n",
    "* Update: $$V_2(s2) = 19.0$$\n",
    "\n",
    "State s3 (R=1):\n",
    "* Up: Moves to s1 ($V_1=5$). Value: $1 + 0.9(5) = 5.5$\n",
    "* Down: Hits wall $\\to$ stays in s3 ($V_1=1$). Value: $1 + 0.9(1) = 1.9$\n",
    "* Left: Hits wall $\\to$ stays in s3 ($V_1=1$). Value: $1 + 0.9(1) = 1.9$\n",
    "* Right: Moves to s4 ($V_1=2$). Value: $1 + 0.9(2) = 2.8$\n",
    "* Max: The best action is Up.\n",
    "* Update: $$V_2(s3) = 5.5$$\n",
    "\n",
    "State s4 (R=2):\n",
    "* Up: Moves to s2 ($V_1=10$). Value: $2 + 0.9(10) = 11.0$\n",
    "* Down: Hits wall $\\to$ stays in s4 ($V_1=2$). Value: $2 + 0.9(2) = 3.8$\n",
    "* Left: Moves to s3 ($V_1=1$). Value: $2 + 0.9(1) = 2.9$\n",
    "* Right: Hits wall $\\to$ stays in s4 ($V_1=2$). Value: $2 + 0.9(2) = 3.8$\n",
    "* Max: The best action is Up.\n",
    "* Update: $$V_2(s4) = 11.0$$\n",
    "\n",
    "The Value Function ($V$) after Second Iteration\n",
    "\n",
    "| State | $V_2(s)$ | Optimal Action (Greedy Policy) |\n",
    "| :--- | :--- | :--- |\n",
    "| **s1** | 14.0 | Right |\n",
    "| **s2** | 19.0 | Up / Right |\n",
    "| **s3** | 5.5 | Up |\n",
    "| **s4** | 11.0 | Up |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ea9646",
   "metadata": {},
   "source": [
    "#### <mark>**Problem 3**</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115022e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Standard Value Iteration...\n",
      "\n",
      "--- Standard Value Iteration ---\n",
      "Optimal Value Function (V*):\n",
      "[[-0.43  0.63  1.81  3.12  4.58]\n",
      " [ 0.63  1.81  3.12  4.58  6.2 ]\n",
      " [ 1.81  3.12  4.58  6.2   8.  ]\n",
      " [ 3.12  4.58  6.2   8.   10.  ]\n",
      " [ 4.58  6.2   8.   10.    0.  ]]\n",
      "\n",
      "Optimal Policy (π*):\n",
      "|  →  |  →  |  →  |  ↓  |  ↓  |\n",
      "|  →  |  →  |  →  |  →  |  ↓  |\n",
      "|  →  |  ↓  |  →  |  →  |  ↓  |\n",
      "|  →  |  →  |  →  |  →  |  ↓  |\n",
      "|  →  |  →  |  →  |  →  |  G  |\n",
      "------------------------------\n",
      "\n",
      "Running In-Place Value Iteration...\n",
      "\n",
      "--- In-Place Value Iteration ---\n",
      "Optimal Value Function (V*):\n",
      "[[-0.43  0.63  1.81  3.12  4.58]\n",
      " [ 0.63  1.81  3.12  4.58  6.2 ]\n",
      " [ 1.81  3.12  4.58  6.2   8.  ]\n",
      " [ 3.12  4.58  6.2   8.   10.  ]\n",
      " [ 4.58  6.2   8.   10.    0.  ]]\n",
      "\n",
      "Optimal Policy (π*):\n",
      "|  →  |  →  |  →  |  ↓  |  ↓  |\n",
      "|  →  |  →  |  →  |  →  |  ↓  |\n",
      "|  →  |  ↓  |  →  |  →  |  ↓  |\n",
      "|  →  |  →  |  →  |  →  |  ↓  |\n",
      "|  →  |  →  |  →  |  →  |  G  |\n",
      "------------------------------\n",
      "\n",
      "=== Performance Comparison ===\n",
      "Method                    | Iterations | Time (s)  \n",
      "--------------------------------------------------\n",
      "Standard VI               | 9          | 0.001741\n",
      "In-Place VI               | 9          | 0.002382\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gridworld_mdp import GridWorldMDP\n",
    "from value_iteration import standard_value_iteration, inplace_value_iteration\n",
    "\n",
    "def print_results(mdp, V, policy, title):\n",
    "    print(f\"\\n--- {title} ---\")\n",
    "    print(\"Optimal Value Function (V*):\")\n",
    "    print(np.round(V, 2))\n",
    "    print(\"\\nOptimal Policy (π*):\")\n",
    "    for r in range(mdp.rows):\n",
    "        row_str = \" | \".join([f\"{str(policy[r, c]):^3}\" for c in range(mdp.cols)])\n",
    "        print(f\"| {row_str} |\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "def main():\n",
    "    mdp = GridWorldMDP(gamma=0.9, theta=1e-4)\n",
    "\n",
    "    # --- Task 1: Standard VI ---\n",
    "    print(\"Running Standard Value Iteration...\")\n",
    "    V_std, iters_std, time_std = standard_value_iteration(mdp)\n",
    "    pi_std = mdp.get_optimal_policy(V_std)\n",
    "    print_results(mdp, V_std, pi_std, \"Standard Value Iteration\")\n",
    "\n",
    "    # --- Task 2: In-Place VI ---\n",
    "    print(\"\\nRunning In-Place Value Iteration...\")\n",
    "    V_in, iters_in, time_in = inplace_value_iteration(mdp)\n",
    "    pi_in = mdp.get_optimal_policy(V_in)\n",
    "    print_results(mdp, V_in, pi_in, \"In-Place Value Iteration\")\n",
    "\n",
    "    # --- Comparison ---\n",
    "    print(\"\\n=== Performance Comparison ===\")\n",
    "    print(f\"{'Method':<25} | {'Iterations':<10} | {'Time (s)':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Standard VI':<25} | {iters_std:<10} | {time_std:.6f}\")\n",
    "    print(f\"{'In-Place VI':<25} | {iters_in:<10} | {time_in:.6f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
